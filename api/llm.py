import asyncio
import fcntl
import json
import os
import random
import time
from pathlib import Path

import openai
import yaml


# ============================================================================
# è·¨è¿›ç¨‹ LLM è°ƒç”¨ç›‘æ§å™¨
# ============================================================================

class CrossProcessLLMMonitor:
    """
    è·¨è¿›ç¨‹ LLM è°ƒç”¨ç›‘æ§å™¨
    
    ä½¿ç”¨æ–‡ä»¶é” + JSON çŠ¶æ€æ–‡ä»¶å®ç°è·¨è¿›ç¨‹çš„å¹¶å‘ç›‘æ§å’Œæ§åˆ¶ã€‚
    æ— è®ºè¿è¡Œå¤šå°‘ä¸ª Python è„šæœ¬ï¼Œéƒ½èƒ½ç»Ÿä¸€è¿½è¸ªç³»ç»Ÿä¸­æ­£åœ¨è¿›è¡Œçš„ LLM è°ƒç”¨æ•°é‡ã€‚
    
    å·¥ä½œåŸç†ï¼š
    1. ä½¿ç”¨ä¸€ä¸ª JSON æ–‡ä»¶è®°å½•æ‰€æœ‰æ´»è·ƒçš„ LLM è°ƒç”¨
    2. ä½¿ç”¨æ–‡ä»¶é”ï¼ˆfcntlï¼‰ä¿è¯å¹¶å‘å®‰å…¨
    3. æ¯ä¸ªè°ƒç”¨å¼€å§‹æ—¶æ³¨å†Œï¼Œç»“æŸæ—¶æ³¨é”€
    4. å®šæœŸæ¸…ç†è¶…æ—¶çš„åƒµå°¸è®°å½•
    """
    
    # é»˜è®¤çŠ¶æ€æ–‡ä»¶è·¯å¾„
    DEFAULT_STATE_FILE = "/tmp/llm_monitor_state.json"
    DEFAULT_LOCK_FILE = "/tmp/llm_monitor.lock"
    
    # è°ƒç”¨è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œè¶…è¿‡è¿™ä¸ªæ—¶é—´çš„è®°å½•ä¼šè¢«æ¸…ç†
    CALL_TIMEOUT = 600  # 10 åˆ†é’Ÿ
    
    # æ¸…ç†é—´éš”ï¼ˆç§’ï¼‰
    CLEANUP_INTERVAL = 30
    
    _instance = None
    _last_cleanup_time = 0
    
    def __new__(cls, state_file=None, lock_file=None):
        """å•ä¾‹æ¨¡å¼"""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self, state_file=None, lock_file=None):
        if self._initialized:
            return
        
        self._state_file = Path(state_file or self.DEFAULT_STATE_FILE)
        self._lock_file = Path(lock_file or self.DEFAULT_LOCK_FILE)
        self._pid = os.getpid()
        self._call_counter = 0  # è¿›ç¨‹å†…è°ƒç”¨è®¡æ•°å™¨
        self._initialized = True
        
        # ç¡®ä¿çŠ¶æ€æ–‡ä»¶å­˜åœ¨
        self._ensure_state_file()
    
    def _ensure_state_file(self):
        """ç¡®ä¿çŠ¶æ€æ–‡ä»¶å­˜åœ¨"""
        if not self._state_file.exists():
            self._write_state({"calls": {}, "stats": {"total_calls": 0}})
    
    def _read_state(self) -> dict:
        """è¯»å–çŠ¶æ€ï¼ˆéœ€è¦åœ¨é”å†…è°ƒç”¨ï¼‰"""
        try:
            if self._state_file.exists():
                with open(self._state_file, 'r') as f:
                    return json.load(f)
        except (json.JSONDecodeError, IOError):
            pass
        return {"calls": {}, "stats": {"total_calls": 0}}
    
    def _write_state(self, state: dict):
        """å†™å…¥çŠ¶æ€ï¼ˆéœ€è¦åœ¨é”å†…è°ƒç”¨ï¼‰"""
        with open(self._state_file, 'w') as f:
            json.dump(state, f)
    
    def _with_lock(self, func):
        """ä½¿ç”¨æ–‡ä»¶é”æ‰§è¡Œæ“ä½œ"""
        # ç¡®ä¿é”æ–‡ä»¶å­˜åœ¨
        self._lock_file.touch(exist_ok=True)
        
        with open(self._lock_file, 'r+') as lock_f:
            fcntl.flock(lock_f.fileno(), fcntl.LOCK_EX)
            try:
                return func()
            finally:
                fcntl.flock(lock_f.fileno(), fcntl.LOCK_UN)
    
    def _cleanup_stale_calls(self, state: dict) -> dict:
        """æ¸…ç†è¶…æ—¶çš„åƒµå°¸è°ƒç”¨è®°å½•"""
        now = time.time()
        calls = state.get("calls", {})
        active_calls = {}
        
        for call_id, call_info in calls.items():
            start_time = call_info.get("start_time", 0)
            if now - start_time < self.CALL_TIMEOUT:
                active_calls[call_id] = call_info
        
        state["calls"] = active_calls
        return state
    
    def _generate_call_id(self) -> str:
        """ç”Ÿæˆå”¯ä¸€çš„è°ƒç”¨ ID"""
        self._call_counter += 1
        return f"{self._pid}_{self._call_counter}_{time.time()}"
    
    def register_call(self, model: str = "unknown") -> str:
        """
        æ³¨å†Œä¸€ä¸ªæ–°çš„ LLM è°ƒç”¨
        
        Returns:
            call_id: ç”¨äºåç»­æ³¨é”€çš„å”¯ä¸€æ ‡è¯†
        """
        call_id = self._generate_call_id()
        
        def _do_register():
            state = self._read_state()
            
            # å®šæœŸæ¸…ç†
            now = time.time()
            if now - CrossProcessLLMMonitor._last_cleanup_time > self.CLEANUP_INTERVAL:
                state = self._cleanup_stale_calls(state)
                CrossProcessLLMMonitor._last_cleanup_time = now
            
            # æ³¨å†Œæ–°è°ƒç”¨
            state["calls"][call_id] = {
                "pid": self._pid,
                "model": model,
                "start_time": now,
            }
            state["stats"]["total_calls"] = state["stats"].get("total_calls", 0) + 1
            
            self._write_state(state)
            return len(state["calls"])
        
        current_count = self._with_lock(_do_register)
        return call_id
    
    def unregister_call(self, call_id: str):
        """æ³¨é”€ä¸€ä¸ª LLM è°ƒç”¨"""
        def _do_unregister():
            state = self._read_state()
            if call_id in state.get("calls", {}):
                del state["calls"][call_id]
                self._write_state(state)
        
        self._with_lock(_do_unregister)
    
    def get_active_count(self) -> int:
        """è·å–å½“å‰æ´»è·ƒçš„ LLM è°ƒç”¨æ•°é‡"""
        def _do_count():
            state = self._read_state()
            state = self._cleanup_stale_calls(state)
            self._write_state(state)
            return len(state.get("calls", {}))
        
        return self._with_lock(_do_count)
    
    def get_status(self) -> dict:
        """
        è·å–è¯¦ç»†çŠ¶æ€ä¿¡æ¯
        
        Returns:
            {
                "active_calls": å½“å‰æ´»è·ƒè°ƒç”¨æ•°,
                "by_process": {pid: count, ...},
                "by_model": {model: count, ...},
                "total_calls": å†å²æ€»è°ƒç”¨æ•°,
                "calls_detail": [{pid, model, duration}, ...]
            }
        """
        def _do_get_status():
            state = self._read_state()
            state = self._cleanup_stale_calls(state)
            self._write_state(state)
            
            calls = state.get("calls", {})
            now = time.time()
            
            by_process = {}
            by_model = {}
            calls_detail = []
            
            for call_id, info in calls.items():
                pid = info.get("pid", "unknown")
                model = info.get("model", "unknown")
                duration = now - info.get("start_time", now)
                
                by_process[pid] = by_process.get(pid, 0) + 1
                by_model[model] = by_model.get(model, 0) + 1
                calls_detail.append({
                    "pid": pid,
                    "model": model,
                    "duration": round(duration, 1),
                })
            
            return {
                "active_calls": len(calls),
                "by_process": by_process,
                "by_model": by_model,
                "total_calls": state.get("stats", {}).get("total_calls", 0),
                "calls_detail": sorted(calls_detail, key=lambda x: -x["duration"]),
            }
        
        return self._with_lock(_do_get_status)
    
    def wait_if_too_many(self, max_concurrent: int, check_interval: float = 0.5) -> int:
        """
        å¦‚æœå½“å‰å¹¶å‘æ•°è¿‡é«˜ï¼Œç­‰å¾…ç›´åˆ°é™åˆ°é˜ˆå€¼ä»¥ä¸‹
        
        Args:
            max_concurrent: æœ€å¤§å…è®¸å¹¶å‘æ•°
            check_interval: æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
        
        Returns:
            å½“å‰æ´»è·ƒè°ƒç”¨æ•°
        """
        while True:
            count = self.get_active_count()
            if count < max_concurrent:
                return count
            time.sleep(check_interval)
    
    async def wait_if_too_many_async(self, max_concurrent: int, check_interval: float = 0.5) -> int:
        """å¼‚æ­¥ç‰ˆæœ¬çš„ç­‰å¾…"""
        while True:
            count = self.get_active_count()
            if count < max_concurrent:
                return count
            await asyncio.sleep(check_interval)


# å…¨å±€ç›‘æ§å™¨å®ä¾‹
_global_monitor = None

def get_llm_monitor() -> CrossProcessLLMMonitor:
    """è·å–å…¨å±€ LLM ç›‘æ§å™¨"""
    global _global_monitor
    if _global_monitor is None:
        _global_monitor = CrossProcessLLMMonitor()
    return _global_monitor


def print_llm_status():
    """æ‰“å°å½“å‰ LLM è°ƒç”¨çŠ¶æ€ï¼ˆæ–¹ä¾¿è°ƒè¯•ï¼‰"""
    monitor = get_llm_monitor()
    status = monitor.get_status()
    
    print("\n" + "=" * 50)
    print(f"ğŸ”¥ LLM è°ƒç”¨ç›‘æ§çŠ¶æ€")
    print("=" * 50)
    print(f"å½“å‰æ´»è·ƒè°ƒç”¨: {status['active_calls']}")
    print(f"å†å²æ€»è°ƒç”¨æ•°: {status['total_calls']}")
    
    if status['by_process']:
        print(f"\næŒ‰è¿›ç¨‹åˆ†å¸ƒ:")
        for pid, count in sorted(status['by_process'].items(), key=lambda x: -x[1]):
            print(f"  PID {pid}: {count} ä¸ªè°ƒç”¨")
    
    if status['by_model']:
        print(f"\næŒ‰æ¨¡å‹åˆ†å¸ƒ:")
        for model, count in sorted(status['by_model'].items(), key=lambda x: -x[1]):
            print(f"  {model}: {count} ä¸ªè°ƒç”¨")
    
    if status['calls_detail']:
        print(f"\næ´»è·ƒè°ƒç”¨è¯¦æƒ… (æŒ‰æ—¶é•¿æ’åº):")
        for i, call in enumerate(status['calls_detail'][:10]):  # æœ€å¤šæ˜¾ç¤º10ä¸ª
            print(f"  [{i+1}] PID={call['pid']}, model={call['model']}, duration={call['duration']}s")
        if len(status['calls_detail']) > 10:
            print(f"  ... è¿˜æœ‰ {len(status['calls_detail']) - 10} ä¸ªè°ƒç”¨")
    
    print("=" * 50 + "\n")


# ============================================================================
# è‡ªé€‚åº”é€Ÿç‡é™åˆ¶å™¨ï¼ˆæ”¯æŒè·¨è¿›ç¨‹æ„ŸçŸ¥ï¼‰
# ============================================================================

class AdaptiveRateLimiter:
    """
    è‡ªé€‚åº”é€Ÿç‡æ§åˆ¶å™¨ - è‡ªåŠ¨æ¢æµ‹æœ€å¤§å¯ç”¨é€Ÿç‡å’Œæœ€å¤§å¹¶å‘æ•°
    
    é‡‡ç”¨ç±»ä¼¼ TCP æ‹¥å¡æ§åˆ¶çš„ AIMD ç­–ç•¥ï¼š
    - æˆåŠŸæ—¶çº¿æ€§å¢åŠ é€Ÿç‡å’Œå¹¶å‘ä¸Šé™ï¼ˆAdditive Increaseï¼‰
    - é‡åˆ° rate limit æ—¶ä¹˜æ³•é™ä½ï¼ˆMultiplicative Decreaseï¼‰
    
    åŒæ—¶ç»´æŠ¤å½“å‰æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡æ•°ï¼ˆin-flight countï¼‰ã€‚
    å¦‚æœå¹¶å‘æ•°æœªè¾¾ä¸Šé™ï¼Œç›´æ¥æ”¾è¡Œï¼›è¾¾åˆ°ä¸Šé™åæ‰å¯ç”¨é€Ÿç‡é™åˆ¶ã€‚
    """
    
    def __init__(
        self,
        initial_rps: float = 50,
        min_rps: float = 5,
        max_concurrent: int = 500,        # åˆå§‹æœ€å¤§å¹¶å‘æ•°
        min_concurrent: int = 50,         # æœ€å°å¹¶å‘æ•°
        burst_allowance: float = 2.0,
        increase_step: float = 5.0,       # æ¯æ¬¡å¢åŠ çš„ RPS
        concurrent_increase_step: int = 10,  # æ¯æ¬¡å¢åŠ çš„å¹¶å‘æ•°
        increase_interval: int = 20,      # æ¯å¤šå°‘æ¬¡æˆåŠŸåå¢åŠ 
        decrease_factor: float = 0.5,     # é‡åˆ° rate limit æ—¶ä¹˜ä»¥çš„ç³»æ•°
    ):
        self._rps = initial_rps
        self._min_rps = min_rps
        self._max_concurrent = max_concurrent
        self._min_concurrent = min_concurrent
        self._burst_allowance = burst_allowance
        self._increase_step = increase_step
        self._concurrent_increase_step = concurrent_increase_step
        self._increase_interval = increase_interval
        self._decrease_factor = decrease_factor
        
        # Token bucket çŠ¶æ€
        self._tokens = initial_rps * burst_allowance
        self._last_refill_time = time.monotonic()
        self._lock = asyncio.Lock()
        
        # å¹¶å‘ä»»åŠ¡è®¡æ•°
        self._in_flight = 0
        self._peak_in_flight = 0
        
        # ç»Ÿè®¡
        self._rate_limit_count = 0
        self._success_count = 0
        self._success_since_last_limit = 0
        
        # å‹åŠ›æ£€æµ‹ï¼šåªæœ‰æœ€è¿‘æœ‰é«˜è´Ÿè½½æ—¶æ‰å¢é€Ÿ
        self._last_pressure_time = 0  # æœ€è¿‘ä¸€æ¬¡é«˜è´Ÿè½½çš„æ—¶é—´
        self._pressure_threshold = 0.5  # è¶…è¿‡ 50% å ç”¨ç‡ç®—æœ‰å‹åŠ›
        self._pressure_window = 10.0  # å‹åŠ›æœ‰æ•ˆçª—å£ï¼ˆç§’ï¼‰
    
    async def acquire(self):
        """è·å–ä»¤ç‰Œï¼ˆæ§åˆ¶å‘é€é€Ÿç‡ï¼‰"""
        async with self._lock:
            # æ£€æµ‹å‹åŠ›ï¼šå¦‚æœå½“å‰è´Ÿè½½è¾ƒé«˜ï¼Œè®°å½•å‹åŠ›æ—¶é—´
            occupancy = self._in_flight / self._max_concurrent if self._max_concurrent > 0 else 0
            if occupancy >= self._pressure_threshold:
                self._last_pressure_time = time.monotonic()
            
            # å¦‚æœå¹¶å‘æ•°æœªè¾¾ä¸Šé™ï¼Œç›´æ¥æ”¾è¡Œ
            if self._in_flight < self._max_concurrent:
                self._in_flight += 1
                if self._in_flight > self._peak_in_flight:
                    self._peak_in_flight = self._in_flight
                return
            
            # è¾¾åˆ°å¹¶å‘ä¸Šé™ï¼Œä½¿ç”¨ token bucket æ§åˆ¶é€Ÿç‡
            now = time.monotonic()
            elapsed = now - self._last_refill_time
            
            # è¡¥å……ä»¤ç‰Œ
            max_tokens = self._rps * self._burst_allowance
            self._tokens = min(max_tokens, self._tokens + elapsed * self._rps)
            self._last_refill_time = now
            
            # å¦‚æœæ²¡æœ‰ä»¤ç‰Œï¼Œç­‰å¾…
            if self._tokens < 1:
                wait_time = (1 - self._tokens) / self._rps
                await asyncio.sleep(wait_time)
                self._tokens = 0
            else:
                self._tokens -= 1
            
            # å¢åŠ å¹¶å‘è®¡æ•°
            self._in_flight += 1
            if self._in_flight > self._peak_in_flight:
                self._peak_in_flight = self._in_flight
    
    def release(self):
        """é‡Šæ”¾ï¼Œå‡å°‘å¹¶å‘è®¡æ•°"""
        self._in_flight = max(0, self._in_flight - 1)
    
    def report_rate_limit(self):
        """
        æŠ¥å‘Š rate limitï¼Œæ™ºèƒ½é™é€Ÿ
        
        æ ¹æ®å½“å‰ in_flight å  max_concurrent çš„æ¯”ä¾‹åˆ¤æ–­ç“¶é¢ˆï¼š
        - é«˜å æ¯”ï¼ˆ>70%ï¼‰ï¼šä¸»è¦é™ä½ max_concurrentï¼Œè½»å¾®é™ä½ RPS
        - ä½å æ¯”ï¼ˆ<30%ï¼‰ï¼šä¸»è¦é™ä½ RPSï¼Œè½»å¾®é™ä½ max_concurrent
        - ä¸­ç­‰å æ¯”ï¼šä¸¤è€…å‡è¡¡é™ä½
        """
        self._rate_limit_count += 1
        old_rps = self._rps
        old_concurrent = self._max_concurrent
        
        # è®¡ç®— in_flight å æ¯”
        occupancy = self._in_flight / self._max_concurrent if self._max_concurrent > 0 else 0
        
        if occupancy > 0.7:
            # å¹¶å‘æ•°å¯èƒ½æ˜¯ç“¶é¢ˆï¼Œä¸»è¦é™ä½å¹¶å‘
            concurrent_factor = 0.7  # é™ 30%
            rps_factor = 0.9         # é™ 10%
            reason = "concurrent bottleneck"
        elif occupancy < 0.3:
            # RPS å¯èƒ½æ˜¯ç“¶é¢ˆï¼Œä¸»è¦é™ä½ RPS
            concurrent_factor = 0.9  # é™ 10%
            rps_factor = 0.7         # é™ 30%
            reason = "RPS bottleneck"
        else:
            # ä¸ç¡®å®šï¼Œå‡è¡¡é™ä½
            concurrent_factor = 0.8  # é™ 20%
            rps_factor = 0.8         # é™ 20%
            reason = "balanced"
        
        self._rps = max(self._min_rps, self._rps * rps_factor)
        self._max_concurrent = max(self._min_concurrent, int(self._max_concurrent * concurrent_factor))
        self._success_since_last_limit = 0  # é‡ç½®æˆåŠŸè®¡æ•°
    
    def report_success(self):
        """
        æŠ¥å‘ŠæˆåŠŸï¼Œæ™ºèƒ½æé€Ÿ
        
        åªæœ‰åœ¨æœ€è¿‘æœ‰å‹åŠ›ï¼ˆé«˜è´Ÿè½½ï¼‰çš„æƒ…å†µä¸‹æ‰å¢é€Ÿï¼Œé¿å…ç©ºé—²æ—¶ç›²ç›®å¢åŠ ã€‚
        æ ¹æ®å½“å‰ in_flight å æ¯”åˆ¤æ–­åº”è¯¥ä¼˜å…ˆå¢åŠ å“ªä¸ªï¼š
        - é«˜å æ¯”ï¼ˆ>80%ï¼‰ï¼šä¼˜å…ˆå¢åŠ  max_concurrent
        - ä½å æ¯”ï¼ˆ<50%ï¼‰ï¼šä¼˜å…ˆå¢åŠ  RPS
        """
        self._success_count += 1
        self._success_since_last_limit += 1
        
        # æ¯ N æ¬¡æˆåŠŸï¼Œè€ƒè™‘å¢åŠ é€Ÿç‡å’Œå¹¶å‘ä¸Šé™
        if self._success_since_last_limit % self._increase_interval == 0:
            # æ£€æŸ¥æœ€è¿‘æ˜¯å¦æœ‰å‹åŠ›ï¼Œæ²¡æœ‰å‹åŠ›å°±ä¸å¢åŠ 
            time_since_pressure = time.monotonic() - self._last_pressure_time
            if time_since_pressure > self._pressure_window:
                # æœ€è¿‘æ²¡æœ‰é«˜è´Ÿè½½ï¼Œä¸éœ€è¦å¢åŠ å®¹é‡
                return
            
            old_rps = self._rps
            old_concurrent = self._max_concurrent
            
            # è®¡ç®— in_flight å æ¯”
            occupancy = self._in_flight / self._max_concurrent if self._max_concurrent > 0 else 0
            
            if occupancy > 0.8:
                # ç»å¸¸è¾¾åˆ°å¹¶å‘ä¸Šé™ï¼Œä¼˜å…ˆå¢åŠ  max_concurrent
                self._max_concurrent += self._concurrent_increase_step * 2
                self._rps += self._increase_step
            elif occupancy < 0.5:
                # å¹¶å‘æ•°ä½ï¼Œä¼˜å…ˆå¢åŠ  RPS
                self._rps += self._increase_step * 2
                self._max_concurrent += self._concurrent_increase_step
            else:
                # å‡è¡¡å¢åŠ 
                self._rps += self._increase_step
                self._max_concurrent += self._concurrent_increase_step
            
            # æ¯è·¨è¶Š 100 çš„å€æ•°æ—¶æ‰“å°æ—¥å¿—
            if int(old_rps / 100) != int(self._rps / 100):
                print(f"[RateLimiter] Increased: RPS {old_rps:.1f} -> {self._rps:.1f}, "
                      f"max_concurrent: {old_concurrent} -> {self._max_concurrent} "
                      f"(in-flight: {self._in_flight}, occupancy={occupancy:.0%})")
    
    async def __aenter__(self):
        await self.acquire()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        self.release()
        return False
    
    @property
    def in_flight(self):
        """å½“å‰æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡æ•°"""
        return self._in_flight
    
    def get_stats(self):
        return {
            "current_rps": self._rps,
            "max_concurrent": self._max_concurrent,
            "in_flight": self._in_flight,
            "peak_in_flight": self._peak_in_flight,
            "rate_limit_count": self._rate_limit_count,
            "success_count": self._success_count,
        }


# ============================================================================
# å…¨å±€é…ç½®ä¸çŠ¶æ€
# ============================================================================

INITIAL_RPS = 1000        # åˆå§‹é€Ÿç‡ï¼ˆæ¿€è¿›èµ·æ­¥ï¼‰
MIN_RPS = 10              # æœ€å°é€Ÿç‡
MAX_CONCURRENT = 800      # åˆå§‹æœ€å¤§å¹¶å‘æ•°ï¼ˆæ¿€è¿›èµ·æ­¥ï¼‰
MIN_CONCURRENT = 50       # æœ€å°å¹¶å‘æ•°ï¼ˆé™é€Ÿä¸‹é™ï¼‰
INCREASE_STEP = 10.0      # æ¯æ¬¡å¢åŠ çš„ RPS
CONCURRENT_INCREASE_STEP = 20  # æ¯æ¬¡å¢åŠ çš„å¹¶å‘æ•°
INCREASE_INTERVAL = 20    # æ¯å¤šå°‘æ¬¡æˆåŠŸåå¢åŠ 
DECREASE_FACTOR = 0.5     # é‡åˆ° rate limit æ—¶ä¹˜ä»¥çš„ç³»æ•°ï¼ˆå¤‡ç”¨ï¼‰
MAX_RETRIES = 3
LOG_TOKEN = True

_global_rate_limiter = None
_client_cache = {}

model_name = None
total_prompt_tokens, total_completion_tokens, call_count = 0, 0, 0
current_prompt_tokens, current_completion_tokens = 0, 0


def load_api_configs():
    """Load API configurations from yaml file"""
    config_path = os.path.join(os.path.dirname(__file__), 'apikey.yaml')
    with open(config_path, 'r', encoding='utf-8') as file:
        return yaml.safe_load(file)


# Load configurations at module import time
config = load_api_configs()
api_configs = {k: v for k, v in config.items() if k != 'model2base'}
model2base = config.get('model2base', {})


# ============================================================================
# è¾…åŠ©å‡½æ•°
# ============================================================================

def _get_client(base_url: str, api_key: str) -> openai.AsyncClient:
    """è·å–æˆ–åˆ›å»ºç¼“å­˜çš„å®¢æˆ·ç«¯"""
    key = (base_url, api_key)
    if key not in _client_cache:
        _client_cache[key] = openai.AsyncClient(base_url=base_url, api_key=api_key)
    return _client_cache[key]


def _parse_response(response, is_chat: bool = True):
    """è§£æå“åº”ï¼Œè¿”å› (content, usage)"""
    if isinstance(response, str):
        return response, None
    if is_chat:
        return response.choices[0].message.content, getattr(response, 'usage', None)
    return response.choices[0].text, getattr(response, 'usage', None)


def _update_tokens(usage, log_token: bool):
    """æ›´æ–° token ç»Ÿè®¡"""
    global current_prompt_tokens, current_completion_tokens
    if LOG_TOKEN and log_token and usage:
        current_prompt_tokens = usage.prompt_tokens
        current_completion_tokens = usage.completion_tokens
        update_token()


def _get_base_and_client(model: str):
    """æ ¹æ®æ¨¡å‹è·å– base é…ç½®å’Œå®¢æˆ·ç«¯"""
    if model in model2base:
        base = model2base[model]
    else:
        base = model2base.get("default", "default")
    if base not in api_configs:
        base = "default"
    
    base_config = api_configs[base]
    api_key = random.choice(base_config["api_key"])
    client = _get_client(base_config["url"], api_key)
    return client


# ============================================================================
# é€Ÿç‡é™åˆ¶å™¨ç®¡ç†
# ============================================================================

def set_rate_limit(initial_rps: float = None, min_rps: float = None,
                   max_concurrent: int = None, min_concurrent: int = None,
                   increase_step: float = None, concurrent_increase_step: int = None,
                   increase_interval: int = None, decrease_factor: float = None):
    """é…ç½®é€Ÿç‡é™åˆ¶å‚æ•°"""
    global INITIAL_RPS, MIN_RPS, MAX_CONCURRENT, MIN_CONCURRENT, INCREASE_STEP, CONCURRENT_INCREASE_STEP, INCREASE_INTERVAL, DECREASE_FACTOR, _global_rate_limiter
    if initial_rps is not None:
        INITIAL_RPS = initial_rps
    if min_rps is not None:
        MIN_RPS = min_rps
    if max_concurrent is not None:
        MAX_CONCURRENT = max_concurrent
    if min_concurrent is not None:
        MIN_CONCURRENT = min_concurrent
    if increase_step is not None:
        INCREASE_STEP = increase_step
    if concurrent_increase_step is not None:
        CONCURRENT_INCREASE_STEP = concurrent_increase_step
    if increase_interval is not None:
        INCREASE_INTERVAL = increase_interval
    if decrease_factor is not None:
        DECREASE_FACTOR = decrease_factor
    _global_rate_limiter = None  # é‡ç½®ï¼Œä¸‹æ¬¡è°ƒç”¨æ—¶ä½¿ç”¨æ–°é…ç½®


async def _get_rate_limiter() -> AdaptiveRateLimiter:
    global _global_rate_limiter
    if _global_rate_limiter is None:
        _global_rate_limiter = AdaptiveRateLimiter(
            initial_rps=INITIAL_RPS,
            min_rps=MIN_RPS,
            max_concurrent=MAX_CONCURRENT,
            min_concurrent=MIN_CONCURRENT,
            increase_step=INCREASE_STEP,
            concurrent_increase_step=CONCURRENT_INCREASE_STEP,
            increase_interval=INCREASE_INTERVAL,
            decrease_factor=DECREASE_FACTOR,
        )
    return _global_rate_limiter


def get_rate_limiter_stats():
    """è·å–å½“å‰é€Ÿç‡é™åˆ¶å™¨ç»Ÿè®¡ä¿¡æ¯"""
    if _global_rate_limiter is not None:
        return _global_rate_limiter.get_stats()
    return None


def reset_rate_limiter():
    """é‡ç½®é€Ÿç‡é™åˆ¶å™¨ï¼ˆé‡æ–°å¼€å§‹æ¢æµ‹ï¼‰"""
    global _global_rate_limiter
    _global_rate_limiter = None


# ============================================================================
# ä¸»è¦ API å‡½æ•°
# ============================================================================

def set_model(model):
    global model_name
    model_name = model
    

def set_log_token(log):
    global LOG_TOKEN
    LOG_TOKEN = log


async def gen(prompt=None, messages=None, model="gpt-4o-mini", temperature=1.0, 
              response_format="text", log_token=True, use_template=True, max_tokens=8192*2,
              stop=None, cross_process_limit: int = None):
    """
    ç”Ÿæˆ LLM å“åº”
    
    Args:
        cross_process_limit: è·¨è¿›ç¨‹å¹¶å‘é™åˆ¶ã€‚å¦‚æœè®¾ç½®ï¼Œä¼šç­‰å¾…å…¨å±€å¹¶å‘æ•°ä½äºæ­¤å€¼æ‰å¼€å§‹è°ƒç”¨ã€‚
                            è¿™ä¸ªé™åˆ¶æ˜¯è·¨æ‰€æœ‰ Python è¿›ç¨‹çš„ï¼
    """
    rate_limiter = await _get_rate_limiter()
    monitor = get_llm_monitor()
    
    # å¦‚æœè®¾ç½®äº†è·¨è¿›ç¨‹é™åˆ¶ï¼Œå…ˆç­‰å¾…
    if cross_process_limit is not None:
        await monitor.wait_if_too_many_async(cross_process_limit)
    
    # æ³¨å†Œè°ƒç”¨
    call_id = monitor.register_call(model=model)
    
    try:
        async with rate_limiter:
            return await _gen_impl(prompt, messages, model, temperature, response_format, 
                                   log_token, use_template, max_tokens, rate_limiter, stop)
    finally:
        # ç¡®ä¿è°ƒç”¨ç»“æŸåæ³¨é”€
        monitor.unregister_call(call_id)


async def _gen_impl(prompt=None, messages=None, model="gpt-4o-mini", temperature=1.0, 
                    response_format="text", log_token=True, use_template=True, 
                    max_tokens=8192*2, rate_limiter=None, stop=None):
    """Core generation logic"""
    global call_count, model_name
    
    if not model:
        model = model_name
    
    client = _get_base_and_client(model)
    errors = []
    retry_base = random.uniform(0.1, 2)
    
    if LOG_TOKEN:
        call_count += 1

    # Text completion mode
    if not use_template:
        if not prompt:
            raise ValueError("Prompt must be provided when use_template=False")
        
        for retry in range(MAX_RETRIES):
            try:
                async with asyncio.timeout(360):
                    content, usage = await _try_completion(client, model, prompt, 
                                                           temperature, max_tokens, response_format)
                    _update_tokens(usage, log_token)
                    if rate_limiter:
                        rate_limiter.report_success()
                    return content, []
                    
            except (asyncio.TimeoutError, openai.RateLimitError, openai.APIError, Exception) as e:
                errors.append(_handle_error(e, rate_limiter))
                print(_format_error_log(errors, retry, MAX_RETRIES))
                await asyncio.sleep(retry_base * (2 ** retry))

        print(_format_error_log(errors, MAX_RETRIES, MAX_RETRIES, is_final=True))
        return None, []
    
    # Chat completion mode
    if not messages:
        if not prompt:
            raise ValueError("Either prompt or messages must be provided")
        messages = [{"role": "user", "content": prompt}]
    elif prompt:
        messages.append({"role": "user", "content": prompt})

    for retry in range(MAX_RETRIES):
        try:
            async with asyncio.timeout(240):
                response = await _chat_completion(client, model, messages, 
                                                   temperature, max_tokens, response_format, stop)
                content, usage = _parse_response(response, is_chat=True)
                _update_tokens(usage, log_token)
                
                if rate_limiter:
                    rate_limiter.report_success()
                
                messages.append({"role": "assistant", "content": content})
                return content, messages
                
        except (asyncio.TimeoutError, openai.RateLimitError, openai.APIError, Exception) as e:
            errors.append(_handle_error(e, rate_limiter))
            print(_format_error_log(errors, retry, MAX_RETRIES))
            await asyncio.sleep(retry_base * (2 ** retry))

    print(_format_error_log(errors, MAX_RETRIES, MAX_RETRIES, is_final=True))
    return None, messages


async def _try_completion(client, model, prompt, temperature, max_tokens, response_format):
    """å°è¯• completion APIï¼Œå¤±è´¥åˆ™å›é€€åˆ° chat API"""
    try:
        response = await client.completions.create(
            model=model,
            prompt=prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            stop=None
        )
        return _parse_response(response, is_chat=False)
    except openai.APIError as e:
        if "OperationNotSupported" in str(e) or "completion operation does not work" in str(e):
            # Fallback to chat API
            messages = [
                {"role": "system", "content": "You are a text completion assistant. Continue the given text naturally without adding any introduction, explanation, or conversation. Just directly continue where the text left off."},
                {"role": "user", "content": f"Continue this text:\n\n{prompt}"}
            ]
            response = await client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                response_format={"type": response_format}
            )
            return _parse_response(response, is_chat=True)
        raise


async def _chat_completion(client, model, messages, temperature, max_tokens, response_format, stop=None):
    """Execute chat completion"""
    # Some models don't support max_tokens parameter
    if model in ["o3-mini", "gpt-5", "gpt-5-mini", "gpt-5-nano"]:
        return await client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            stop=stop,
            response_format={"type": response_format}
        )
    return await client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
        stop=stop,
        max_tokens=max_tokens,
        response_format={"type": response_format}
    )


def _handle_error(e, rate_limiter) -> str:
    """å¤„ç†é”™è¯¯å¹¶è¿”å›é”™è¯¯æè¿°"""
    if isinstance(e, asyncio.TimeoutError):
        return "Timeout"
    if isinstance(e, openai.RateLimitError):
        if rate_limiter:
            rate_limiter.report_rate_limit()
        return "RateLimit"
    if isinstance(e, openai.APIError):
        # æå–å…³é”®é”™è¯¯ä¿¡æ¯ï¼Œå»é™¤å†—ä½™å†…å®¹
        err_str = str(e)
        if len(err_str) > 100:
            err_str = err_str[:100] + "..."
        return f"API({err_str})"
    return f"{type(e).__name__}"


def _format_error_log(errors: list, retry: int, max_retries: int, is_final: bool = False) -> str:
    """æ ¼å¼åŒ–é”™è¯¯æ—¥å¿—è¾“å‡º"""
    if is_final:
        return f"[LLM] âœ— Failed after {max_retries} retries | Errors: {' â†’ '.join(errors)}"
    else:
        return f"[LLM] Retry {retry + 1}/{max_retries} | {errors[-1] if errors else 'Unknown'}"


# ============================================================================
# Token ç»Ÿè®¡å‡½æ•°
# ============================================================================

def update_token():
    global total_prompt_tokens, total_completion_tokens
    total_prompt_tokens += current_prompt_tokens
    total_completion_tokens += current_completion_tokens


def reset_token():
    global total_prompt_tokens, total_completion_tokens, call_count
    total_prompt_tokens = 0
    total_completion_tokens = 0
    call_count = 0


def get_model():
    return model_name


def get_token():
    return total_prompt_tokens, total_completion_tokens


def get_call_count():
    return call_count


def get_current_tokens():
    return current_prompt_tokens, current_completion_tokens


def reset_current_tokens():
    global current_prompt_tokens, current_completion_tokens
    current_prompt_tokens = 0
    current_completion_tokens = 0


# ============================================================================
# è·¨è¿›ç¨‹ç›‘æ§ä¾¿æ·å‡½æ•°
# ============================================================================

def get_system_llm_count() -> int:
    """è·å–ç³»ç»Ÿä¸­å½“å‰æ­£åœ¨è¿›è¡Œçš„ LLM è°ƒç”¨æ€»æ•°ï¼ˆè·¨æ‰€æœ‰è¿›ç¨‹ï¼‰"""
    return get_llm_monitor().get_active_count()


def get_system_llm_status() -> dict:
    """è·å–ç³»ç»Ÿä¸­ LLM è°ƒç”¨çš„è¯¦ç»†çŠ¶æ€"""
    return get_llm_monitor().get_status()


def set_cross_process_limit(max_concurrent: int):
    """
    è®¾ç½®è·¨è¿›ç¨‹å¹¶å‘é™åˆ¶
    
    è°ƒç”¨æ­¤å‡½æ•°åï¼Œæ‰€æœ‰é€šè¿‡ gen() å‘èµ·çš„è°ƒç”¨éƒ½ä¼šéµå®ˆè¿™ä¸ªå…¨å±€é™åˆ¶
    """
    global CROSS_PROCESS_LIMIT
    CROSS_PROCESS_LIMIT = max_concurrent


# å…¨å±€è·¨è¿›ç¨‹é™åˆ¶ï¼ˆNone è¡¨ç¤ºä¸é™åˆ¶ï¼‰
CROSS_PROCESS_LIMIT = None


# ============================================================================
# å‘½ä»¤è¡Œå·¥å…·ï¼šç›‘æ§ç³»ç»Ÿä¸­çš„ LLM è°ƒç”¨
# ============================================================================

def _cli_monitor():
    """å‘½ä»¤è¡Œç›‘æ§å·¥å…·å…¥å£"""
    import argparse
    import sys
    
    parser = argparse.ArgumentParser(description="LLM è°ƒç”¨è·¨è¿›ç¨‹ç›‘æ§å·¥å…·")
    parser.add_argument("--watch", "-w", action="store_true", 
                        help="æŒç»­ç›‘æ§æ¨¡å¼ï¼Œæ¯ç§’åˆ·æ–°")
    parser.add_argument("--interval", "-i", type=float, default=1.0,
                        help="ç›‘æ§åˆ·æ–°é—´éš”ï¼ˆç§’ï¼‰")
    parser.add_argument("--json", "-j", action="store_true",
                        help="ä»¥ JSON æ ¼å¼è¾“å‡º")
    
    args = parser.parse_args()
    
    monitor = get_llm_monitor()
    
    if args.watch:
        # æŒç»­ç›‘æ§æ¨¡å¼
        try:
            while True:
                if not args.json:
                    # æ¸…å±
                    print("\033[2J\033[H", end="")
                    print_llm_status()
                else:
                    status = monitor.get_status()
                    print(json.dumps(status))
                time.sleep(args.interval)
        except KeyboardInterrupt:
            print("\nç›‘æ§ç»“æŸ")
    else:
        # å•æ¬¡è¾“å‡º
        if args.json:
            status = monitor.get_status()
            print(json.dumps(status, indent=2))
        else:
            print_llm_status()


if __name__ == "__main__":
    _cli_monitor()
